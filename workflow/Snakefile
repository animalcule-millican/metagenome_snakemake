import os

configfile: "/home/glbrc.org/millican/repos/metagenome_snakemake/workflow/config.yml"
workdir: config["working_directory"]


def calc_chunks(fasta):
    import gzip
    import math
    with gzip.open(fasta, "rt") as f:
        lines = f.readlines()
    chunk = math.ceil((len(lines)/2)/12)
    if chunk > 2000:
        return 2000
    else:
        return chunk

def get_mags(dir_path):
    import os
    files = os.listdir(dir_path)
    bin_list = [file.replace(".fna.gz", "") for file in files if file.replace(".fna.gz", "")]
    return bin_list

def create_tmpdir():
    import random
    import os
    import pickle
    with open("/home/glbrc.org/millican/repos/metagenome_snakemake/etc/adj-aml.pkl", 'rb') as f:
        adj, aml = pickle.load(f)
    temp_dir_base = "/home/glbrc.org/millican/TMPDIR"    # Replace with the base path for temporary directories
    # Construct the temporary directory path
    tmpdir = os.path.join(temp_dir_base, f"{random.choice(adj)}-{random.choice(aml)}")
    # Check if the directory exists, and find a new combination if it does
    while os.path.exists(tmpdir):
        tmpdir = os.path.join(temp_dir_base, f"{random.choice(adj)}-{random.choice(aml)}")
    # Once we find a combination that does not already exist
    # Create the temporary directory
    os.makedirs(tmpdir, exist_ok=True)
    return tmpdir

#include: "rules/quantify_reads.smk"
#include: "rules/mags.smk"
#include: "rules/process_reads.smk"
#include: "rules/annotate.smk"

bin_names = get_mags(config["mag_directory"])
names = config["sample_name"]

wildcard_constraints:
    sample_name = "|".join(names),
    data_dir = config["data_directory"],
    bin_name = "|".join(bin_names),
    kmer = "|".join(config["kmer"])


rule all:
    input:
        expand("{data_dir}/gather/{sample_name}_k{kmer}.mag_gather.csv", data_dir=config["data_directory"], sample_name=config["sample_name"], kmer=config["kmer"]),
        expand("{data_dir}/count/{sample_name}.count", data_dir=config["data_directory"], sample_name=config["sample_name"]),
        #expand("{data_dir}/deepbgc/{sample_name}.bgc.gbk", data_dir=config["data_directory"], sample_name=config["sample_name"]),
        expand("{data_dir}/deepbgc/{bin_name}.bgc.gbk", data_dir=config["data_directory"], bin_name=bin_names),
        expand("{data_dir}/metabolic_model/{bin_name}-all-Pathways.tbl", data_dir=config["data_directory"], bin_name=bin_names)

rule sketch_mags:
    input:
        expand("{data_dir}/bins/{bin_name}.fna.gz", bin_name=bin_names, data_dir=config["data_directory"])
    output:
        "{data_dir}/sketch/mags.zip"
    params:
        mag = config["mag_directory"],
        csv = "{data_dir}/mag_list.csv"
    threads: 12
    resources:
        mem_mb = 20000
    conda:
        "branchwater"
    shell:
        """
        scripts/sketch_mags.py {params.mag} {params.csv} {output} {threads}
        """

rule sketch_reads:
    input:
        "{data_dir}/reads/{sample_name}.fastq.gz"
    output:
        "{data_dir}/reads/{sample_name}.sig.gz"
    params:
        param = "k=21,k=31,k=51,scaled=1000,abund",
        sample = "{sample_name}"
    threads: 1
    resources:
        mem_mb = 10000
    conda:
        "branchwater"
    shell:
        """
        sourmash sketch dna {input} -o {output} -p {params.param} --name {params.sample}
        """

rule fastgather_mags:
    input:
        sample = "{data_dir}/reads/{sample_name}.sig.gz",
        mag = "{data_dir}/sketch/mags.zip"
    output:
        "{data_dir}/gather/{sample_name}_k{kmer}.mags.csv"
    params:
        kmer = "{kmer}",
        threads = 12
    resources:
        mem_mb = 20000
    conda:
        "branchwater"
    shell:
        """
        sourmash scripts fastgather -o {output} -t 10000 -k {params.kmer} -c {threads} {input.sample} {input.mag}
        """

rule gather_mags:
    input:
        sample = "{data_dir}/reads/{sample_name}.sig.gz",
        mag = "{data_dir}/sketch/mags.zip",
        gather = "{data_dir}/gather/{sample_name}_k{kmer}.mags.csv"
    output:
        "{data_dir}/gather/{sample_name}_k{kmer}.mag_gather.csv"
    params:
        kmer = "{kmer}",
    threads: 1
    resources:
        mem_mb = 10000
    conda:
        "branchwater"
    shell:
        """
        sourmash gather {input.sample} {input.mag} --picklist {input.gather}:match_name:ident -o {output} -k {params.kmer} --threshold-bp 10000
        """

rule gene_pred:
    input:
        "{data_dir}/reads/{sample_name}.fastq.gz"
    output:
        prot = "{data_dir}/proteins/{sample_name}.faa",
        nucl = "{data_dir}/genes/{sample_name}.fna",
        gff = "{data_dir}/orf/{sample_name}.gff"
    params:
        chunks = lambda wildcards: calc_chunks(wildcards.data_dir + "/reads/" + wildcards.sample_name + ".fastq.gz")
    threads: 12
    resources:
        mem_mb = 30000,
        tmpdir = create_tmpdir()
    conda:
        "genepred"
    shell:
        """
        pprodigal -i {input} -o {output.gff} -a {output.prot} -d {output.nucl} -f gff -T 12 -C {params.chunks} -p meta
        """

rule mag_genepred:
    input:
        "{data_dir}/bins/{bin_name}.fna.gz"
    output:
        prot = "{data_dir}/proteins/{bin_name}.faa",
        nucl = "{data_dir}/genes/{bin_name}.fna",
        gff = "{data_dir}/orf/{bin_name}.gff"
    params:
        chunks = lambda wildcards: calc_chunks(wildcards.data_dir + "/bins/" + wildcards.bin_name + ".fna.gz")
    threads: 12
    resources:
        mem_mb = 10000,
        tmpdir = create_tmpdir()
    conda:
        "genepred"
    shell:
        """
        pprodigal -i {input} -o {output.gff} -a {output.prot} -d {output.nucl} -f gff -T 12 -C {params.chunks}
        """

rule index:
    input:
        config["co_assembly"]
    output:
        "{data_dir}/index/co_assembly_index.1.bt2",
        "{data_dir}/index/co_assembly_index.2.bt2",
        "{data_dir}/index/co_assembly_index.3.bt2",
        "{data_dir}/index/co_assembly_index.4.bt2",
        "{data_dir}/index/co_assembly_index.rev.1.bt2",
        "{data_dir}/index/co_assembly_index.rev.2.bt2"
    params:
        "{data_dir}/index/co_assembly_index"
    threads: 12
    resources:
        mem_mb = 300000,
        tmpdir = create_tmpdir()
    conda:
        "read-mapping"
    shell:
        """
        bowtie2-build -f --threads {threads} {input} {params}
        """

rule map_reads:
    input:
        reads = "{data_dir}/reads/{sample_name}.fastq.gz",
        in1 = "{data_dir}/index/co_assembly_index.1.bt2",
        in2 = "{data_dir}/index/co_assembly_index.2.bt2",
        in3 = "{data_dir}/index/co_assembly_index.3.bt2",
        in4 = "{data_dir}/index/co_assembly_index.4.bt2",
        in5 = "{data_dir}/index/co_assembly_index.rev.1.bt2",
        in6 = "{data_dir}/index/co_assembly_index.rev.2.bt2"
    output:
        "{data_dir}/map/{sample_name}.sorted.bam"
    params:
        "{data_dir}/index/co_assembly_index"
    threads: 16
    resources:
        mem_mb = 300000,
        tmpdir = create_tmpdir()
    conda:
        "read-mapping"
    shell:
        """
        bowtie2 -x {params} --interleaved {input.reads} -p {threads} | samtools view -bS - | samtools sort -@ {threads} - > {output}
        """

rule index_bam:
    input:
        "{data_dir}/map/{sample_name}.sorted.bam"
    output:
        "{data_dir}/map/{sample_name}.sorted.bam.bai"
    threads: 16
    resources:
        mem_mb = 60000,
        tmpdir = create_tmpdir()
    conda:
        "read-mapping"
    shell:
        """
        samtools index -@ {threads} -o {output} {input}
        """

rule count_features:
    input:
        bam = "{data_dir}/map/{sample_name}.sorted.bam",
        idx = "{data_dir}/map/{sample_name}.sorted.bam.bai"
    output:
        "{data_dir}/count/{sample_name}.count"
    params:
        annotation = "{data_dir}/annotation/{sample_name}_functional_annotation.gff"
    threads: 16
    resources:
        mem_mb = 40000,
        tmpdir = create_tmpdir()
    conda:
        "read-mapping"
    shell:
        """
        featureCounts -T {threads} -p --countReadPairs --tmpDir {resources.tmpdir} -t CDS -g ID -F GFF -a {params.annotation} -o {output} {input.bam}
        """

rule mags_deepbgc:
    input: 
        "{data_dir}/bins/{bin_name}.fna.gz"
    output:
        "{data_dir}/deepbgc/{bin_name}.bgc.gbk",
        "{data_dir}/deepbgc/{bin_name}.bgc.tsv",
        "{data_dir}/deepbgc/{bin_name}.full.gbk",
        "{data_dir}/deepbgc/{bin_name}.pfam.tsv",
        "{data_dir}/deepbgc/{bin_name}.antismash.json"
    params:
        dir = "{data_dir}/deepbgc"
    threads: 12
    resources:
        mem_mb = 20000,
        tmpdir = create_tmpdir()
    conda:
        "deepbgc"
    shell:
        """
        deepbgc pipeline -d clusterfinder_retrained -d clusterfinder_original -d clusterfinder_geneborder -d deepbgc --output {params} --label clf_ret --label clf_og --label clf_gb --label deep -c product_activity -c product_class {input}
        """

rule mag_bgc_convert:
    input:
        "{data_dir}/deepbgc/{bin_name}.bgc.tsv"
    output:
        "{data_dir}/deepbgc/{bin_name}.bigslice.bgc.csv"
    threads: 1
    resources:
        mem_mb = 10000
    conda:
        "deepbgc"
    shell:
        """
        convert_deepbgc_output.sh {input} {output}
        """



rule sample_deepbgc:
    input:
        "{data_dir}/assembly/{sample_name}.fna.gz"
    output:
        "{data_dir}/deepbgc/{sample_name}.bgc.gbk",
        "{data_dir}/deepbgc/{sample_name}.bgc.tsv",
        "{data_dir}/deepbgc/{sample_name}.full.gbk",
        "{data_dir}/deepbgc/{sample_name}.pfam.tsv",
        "{data_dir}/deepbgc/{sample_name}.antismash.json"
    params:
        dir = "{data_dir}/deepbgc"
    threads: 12
    resources:
        mem_mb = 30000,
        tmpdir = create_tmpdir()
    conda:
        "deepbgc"
    shell:
        """
        deepbgc pipeline --prodigal-meta-mode -d clusterfinder_retrained -d clusterfinder_original -d clusterfinder_geneborder -d deepbgc --output {params} --label clf_ret --label clf_og --label clf_gb --label deep -c product_activity -c product_class {input}
        """

rule mag_gapseq:
    input:
        "{data_dir}/proteins/{bin_name}.faa"
    output:
        rxn = "{data_dir}/metabolic_model/{bin_name}-all-Reactions.tbl",
        trans = "{data_dir}/metabolic_model/{bin_name}-Transporter.tbl",
        path = "{data_dir}/metabolic_model/{bin_name}-all-Pathways.tbl",
        draft = "{data_dir}/metabolic_model/{bin_name}-draft.RDS",
        wgh = "{data_dir}/metabolic_model/{bin_name}-rxnWeights.RDS",
        xgen = "{data_dir}/metabolic_model/{bin_name}-rxnXgenes.RDS",
    params:
        media = config["media"],
        dirpath = "{data_dir}/metabolic_model"
    threads: 8
    resources:
        mem_mb = 10000,
        tmpdir = create_tmpdir()
    conda:
        "gapseq"
    shell:
        """
        gapseq find -p all -M prot -i 30 -c 60 {input} -T {resources.tmpdir} -K {threads} -f {params.dirpath}
        gapseq find-transport -M prot -K {threads} -f {params.dirpath} {input}
        gapseq draft -r {output.rxn} -t {output.rxn} -p {output.rxn} -c {input} -f {params.dirpath}
        gapseq fill -m {output.rxn} -c {output.rxn} -g {output.rxn} -n {params.media} -f {params.dirpath}
        """
